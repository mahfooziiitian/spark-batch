"""
Event-time is the time embedded in the data itself.

For many applications, you may want to operate on this event-time.

For example, if you want to get the number of events generated by IoT devices every minute, then you probably want to
use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them.

This event-time is very naturally expressed in this model – each event from the devices is a row in the table, and
event-time is a column value in the row.

This allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and
aggregation on the event-time column – each time window is a group and each row can belong to multiple windows/groups.

Therefore, such event-time-window-based aggregation queries can be defined consistently on both a static dataset
(e.g. from collected device events logs) and on a data stream, making the life of the user much easier.

Furthermore, this model naturally handles data that has arrived later than expected based on its event-time.

Since Spark is updating the Result Table, it has full control over updating old aggregates when there is late data,
as well as cleaning up old aggregates to limit the size of intermediate state data.

Since Spark 2.1, we have support for watermarking which allows the user to specify the threshold of late data,
and allows the engine to accordingly clean up old state.

"""
