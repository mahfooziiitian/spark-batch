/*

The Spark Worker is one of many physical nodes (machines) in the cluster.
It is responsible to launch Executors on which Sparkâ€™s tasks run.
Typically, only a single worker runs per node in all its various deployment modes.
Any node that can run application code in the cluster.
Each worker offers resources (memory, CPU, etc.) to the cluster manager and performs the assigned work.
 */
package com.mahfooz.spark.framework.worker

class SparkWorker {

}
