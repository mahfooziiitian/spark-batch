#Defining Agent-1 and the logical names of the Source/ Channel and Sink
a1.sources = src-1
a1.channels = c1
a1.sinks = spark

#Defining Agent-2 and the logical names of the Source/ Channel and Sink
a2.sources = src-2
a2.channels = c2
a2.sinks = spark1

#Configuring Source for Agent-1
#Here we are defining a source which will execute a custom Linux Command "tail" to get the Data from configured web log file
a1.sources.src-1.type = exec
#Name of the Log File with the full path
a1.sources.src-1.command = tail -f /home/servers/node-1/appserver-1/logs/debug.log
#Define the Channel which will be used by Source to deliver the messages.
a1.sources.src-1.channels = c1

#Defining and providing Configuration of Channel for Agent-1
#Memory channel is not a reliable channel.
a1.channels.c1.type = memory
a1.channels.c1.capacity = 2000
a1.channels.c1.transactionCapacity = 100

#Configuring Sink for Agent-1
a1.sinks.spark.type = spark
#This is the Custom Sink which will be used to integrate with our Spark Application
a1.sinks.spark.type = org.apache.spark.streaming.flume.sink.SparkSink
#Name of the host where this Sink is running
a1.sinks.spark.hostname = localhost
#Custom port where our Spark-Application will connect and consume the event
a1.sinks.spark.port = 4949
#Define the Channel which will be used by Sink to receive the messages.
a1.sinks.spark.channel = c1

#Configuring Source for Agent-2
#Here we are defining a source which will execute a custom Linux Command "tail" to get the Data from configured web log file
a2.sources.src-2.type = exec
#Name of the Log File with the full path
a2.sources.src-2.command = tail -f /home/servers/node-1/appserver-2/logs/debug.log
#Define the Channel which will be used by Source to deliver the messages.
a2.sources.src-2.channels = c2

#Defining and providing Configuration of Channel for Agent-2
a2.channels.c2.type = memory
a2.channels.c2.capacity = 2000
a2.channels.c2.transactionCapacity = 100

#Configuring Sink for Agent-2
a2.sinks.spark1.type = spark
#This is the Custom Sink which will be used to integrate with our Spark Application
a2.sinks.spark1.type = org.apache.spark.streaming.flume.sink.SparkSink
#Name of the host where this Sink is running
a2.sinks.spark1.hostname = localhost
#Custom port where our Spark-Application will connect and consume the event
a2.sinks.spark1.port = 4950
#Define the Channel which will be used by Sink to receive the messages.
a2.sinks.spark1.channel = c2
