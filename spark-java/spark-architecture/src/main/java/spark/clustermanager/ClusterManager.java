/*

Spark uses a cluster manager to acquire cluster resources for executing a job.

A cluster manager, as the name implies, manages computing resources across a cluster of worker nodes.

It provides low-level scheduling of cluster resources across applications.

It enables multiple applications to share cluster resources and run on the same worker nodes.

Spark currently supports three cluster managers: standalone, Mesos, and YARN.

Mesos and YARN allow you to run Spark and Hadoop applications simultaneously on the same worker nodes.


 */
package spark.clustermanager;

public class ClusterManager {
}
