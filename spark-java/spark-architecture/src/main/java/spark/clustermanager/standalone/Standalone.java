/*

Spark Standalone deployment means Spark occupies the place on top of HDFS(Hadoop Distributed File System) and space is allocated for HDFS, explicitly.

Here, Spark and MapReduce will run side by side to cover all spark jobs on cluster.

 */
package spark.clustermanager.standalone;

public class Standalone {

}
